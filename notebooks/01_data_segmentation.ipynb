{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c317a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "'''Cell 1: Imports and Setup'''\n",
    "# ==============================================================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74e9565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: f:\\Prof. Nick\n",
      "Metadata file: f:\\Prof. Nick\\data\\raw\\Coupon_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "'''Cell 2: Configuration and Path Setup'''\n",
    "# ==============================================================================\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "RAW_ROOT_DIR = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "INTERIM_ROOT_DIR = os.path.join(PROJECT_ROOT, 'data', 'interim')\n",
    "METADATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'raw', 'Coupon_metadata.csv') # State metadata table csv here\n",
    "\n",
    "BOX_SIZE = 14.0\n",
    "ANGLE_DEG = 10\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Metadata file: {METADATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dba7e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and processed Coupon_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "'''Cell 3: Load Full Metadata'''\n",
    "# ==============================================================================\n",
    "try:\n",
    "    metadata_df = pd.read_csv(METADATA_PATH)\n",
    "    metadata_df.columns = metadata_df.columns.str.strip()\n",
    "    '''\n",
    "    metadata_df.rename(columns={\n",
    "        'Coupon': 'coupon_id',\n",
    "        'Center_X': 'midpoint_x',\n",
    "        'Center_Y': 'midpoint_y'\n",
    "    }, inplace=True)\n",
    "    '''\n",
    "    \n",
    "    print(\"✅ Successfully loaded and processed Coupon_metadata.csv\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Metadata file not found at '{METADATA_PATH}'\")\n",
    "    metadata_df = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading metadata: {e}\")\n",
    "    metadata_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c141cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cell 4: Main Segmentation Loop'''\n",
    "# ==============================================================================\n",
    "def segment_all_builds(raw_root, interim_root, full_metadata, box_size, angle_deg):\n",
    "    if full_metadata is None:\n",
    "        print(\"Aborting: Metadata not loaded.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        build_pattern = re.compile(r'^Build_(\\d+)', re.IGNORECASE)\n",
    "        build_dirs = sorted([d for d in os.listdir(raw_root) if os.path.isdir(os.path.join(raw_root, d)) and build_pattern.match(d)])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Raw data root directory not found: {raw_root}\")\n",
    "        return\n",
    "\n",
    "    if not build_dirs:\n",
    "        print(\"❌ No 'Build_...' subfolders found in 'data/raw/'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(build_dirs)} builds to process: {build_dirs}\")\n",
    "    \n",
    "    for build_dir_name in build_dirs:\n",
    "        \n",
    "        match = build_pattern.match(build_dir_name)\n",
    "        if not match:\n",
    "            continue\n",
    "        \n",
    "        build_num = int(match.group(1))\n",
    "        build_id_from_folder = f'B{build_num}'\n",
    "        \n",
    "        raw_data_dir = os.path.join(raw_root, build_dir_name)\n",
    "        interim_dir = os.path.join(interim_root, f\"{build_id_from_folder}_segmented\")\n",
    "        os.makedirs(interim_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n--- Starting processing for: {build_id_from_folder} ---\")\n",
    "\n",
    "        build_metadata = full_metadata[full_metadata['Build_id'] == build_id_from_folder].copy()\n",
    "        \n",
    "        if build_metadata.empty:\n",
    "            print(f\"⚠️ Warning: No metadata found for {build_id_from_folder}. Skipping this build.\")\n",
    "            continue\n",
    "            \n",
    "        build_metadata.set_index('Coupon', inplace=True)\n",
    "        coupon_midpoint_map = build_metadata[['Center_X', 'Center_Y']].to_dict('index')\n",
    "\n",
    "        segment_single_build(raw_data_dir, interim_dir, coupon_midpoint_map, box_size, angle_deg)\n",
    "\n",
    "def segment_single_build(raw_dir, output_dir, midpoints, box_size, angle_deg):\n",
    "    \"\"\"\n",
    "    Processes all layer files for a single build and saves them into\n",
    "    coupon-specific subdirectories.\n",
    "    \"\"\"\n",
    "    half_box = box_size / 2\n",
    "    angle_rad = np.deg2rad(-angle_deg)\n",
    "    cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)\n",
    "\n",
    "    txt_files = [f for f in os.listdir(raw_dir) if f.lower().endswith(\".txt\")]\n",
    "    txt_files.sort(key=lambda f: int(re.search(r'(\\d+)', f).group()))\n",
    "    \n",
    "    for txt_file in tqdm(txt_files, desc=f\"  Segmenting {os.path.basename(raw_dir)}\"):\n",
    "        try:\n",
    "            layer_num = int(re.search(r'(\\d+)', txt_file).group(1))\n",
    "            \n",
    "            full_layer_df = pd.read_csv(os.path.join(raw_dir, txt_file), sep='\\t', header=0)\n",
    "            full_layer_df.columns = full_layer_df.columns.str.replace(r\"\\s*\\(mean\\)\", \"\", regex=True).str.strip()\n",
    "            \n",
    "            cols_to_use = [\"Demand X\", \"Demand Y\", \"MeltVIEW plasma\", \"MeltVIEW melt pool\", \"LaserVIEW\"]\n",
    "            layer_df = full_layer_df[cols_to_use]\n",
    "\n",
    "            for coupon_id, midpoint in midpoints.items():\n",
    "                # --- CHANGE: Create a subfolder for each coupon ---\n",
    "                coupon_output_dir = os.path.join(output_dir, coupon_id)\n",
    "                os.makedirs(coupon_output_dir, exist_ok=True)\n",
    "                \n",
    "                # Use the correct keys ('Center_X', 'Center_Y') from the dictionary\n",
    "                x_mid, y_mid = midpoint['Center_X'], midpoint['Center_Y']\n",
    "                \n",
    "                dx = layer_df[\"Demand X\"] - x_mid\n",
    "                dy = layer_df[\"Demand Y\"] - y_mid\n",
    "                x_rot = dx * cos_a - dy * sin_a\n",
    "                y_rot = dx * sin_a + dy * cos_a\n",
    "\n",
    "                mask = (\n",
    "                    (x_rot >= -half_box) & (x_rot <= half_box) &\n",
    "                    (y_rot >= -half_box) & (y_rot <= half_box)\n",
    "                )\n",
    "                \n",
    "                coupon_zone_df = layer_df[mask]\n",
    "\n",
    "                if not coupon_zone_df.empty:\n",
    "                    # Save the segmented file into its coupon-specific folder\n",
    "                    save_filename = f\"layer_{layer_num:04d}.csv\" # Simplified filename\n",
    "                    save_path = os.path.join(coupon_output_dir, save_filename)\n",
    "                    coupon_zone_df.to_csv(save_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ Could not process {txt_file}. Error: {e}\")\n",
    "            \n",
    "    print(f\"✅ Segmentation complete for this build.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fda6fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 builds to process: ['Build_01_SS316LCubes_26_02_2025', 'Build_02_SS316LCubes_28_02_2025', 'Build_03_SS316LCubes_04_03_2025', 'Build_04_SS316LCubes_05_03_2025', 'Build_05_SS316LCubes_06_03_2025']\n",
      "\n",
      "--- Starting processing for: B1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Segmenting Build_01_SS316LCubes_26_02_2025: 100%|██████████| 183/183 [11:38<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Segmentation complete for this build.\n",
      "\n",
      "--- Starting processing for: B2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Segmenting Build_02_SS316LCubes_28_02_2025: 100%|██████████| 366/366 [30:16<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Segmentation complete for this build.\n",
      "\n",
      "--- Starting processing for: B3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Segmenting Build_03_SS316LCubes_04_03_2025: 100%|██████████| 366/366 [31:35<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Segmentation complete for this build.\n",
      "\n",
      "--- Starting processing for: B4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Segmenting Build_04_SS316LCubes_05_03_2025: 100%|██████████| 365/365 [38:13<00:00,  6.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Segmentation complete for this build.\n",
      "\n",
      "--- Starting processing for: B5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Segmenting Build_05_SS316LCubes_06_03_2025: 100%|██████████| 366/366 [39:52<00:00,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Segmentation complete for this build.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Cell 5: Run the Full Automated Process'''\n",
    "# ==============================================================================\n",
    "if metadata_df is not None:\n",
    "    segment_all_builds(RAW_ROOT_DIR, INTERIM_ROOT_DIR, metadata_df, BOX_SIZE, ANGLE_DEG)\n",
    "else:\n",
    "    print(\"Could not run process because metadata was not loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
