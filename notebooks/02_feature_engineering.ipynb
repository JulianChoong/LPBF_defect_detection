{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06526c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cell 1: Imports'''\n",
    "# ==============================================================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.cluster import DBSCAN\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc6bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cell 2: Configuration and Path Setup'''\n",
    "# ==============================================================================\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "INTERIM_ROOT_DIR = os.path.join(PROJECT_ROOT, 'data', 'interim')\n",
    "PROCESSED_DIR = os.path.join(PROJECT_ROOT, 'data', 'processed')\n",
    "METADATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'raw', 'Coupon_metadata.csv')\n",
    "OUTPUT_CSV_PATH = os.path.join(PROCESSED_DIR, 'dataset.csv')\n",
    "\n",
    "GRID_SIZE = 300\n",
    "SENSOR_CHANNELS = ['MeltVIEW plasma', 'MeltVIEW melt pool', 'LaserVIEW']\n",
    "GLCM_LEVELS = 64\n",
    "GLCM_DISTANCES = [4, 8]\n",
    "GLCM_ANGLES = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "DBSCAN_HOTSPOT_PERCENTILE = 95\n",
    "DBSCAN_COLDSPOT_PERCENTILE = 5\n",
    "DBSCAN_EPS = 10.0\n",
    "DBSCAN_MIN_SAMPLES = 50\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "print(f\"Reading interim data from: {INTERIM_ROOT_DIR}\")\n",
    "print(f\"Saving final dataset to: {OUTPUT_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cell 3: Feature Calculation Helper Functions '''\n",
    "# ==============================================================================\n",
    "def safe_divide(a, b):\n",
    "    \"\"\"Division with zero protection.\"\"\"\n",
    "    return a / b if abs(b) > 1e-9 else 0.0\n",
    "\n",
    "# --- FIX: Restored the missing calculate_entropy function ---\n",
    "def calculate_entropy(data, bins=32):\n",
    "    \"\"\"Calculate Shannon entropy of a 1D array.\"\"\"\n",
    "    # Remove NaN values to avoid errors\n",
    "    clean_data = data[~np.isnan(data)]\n",
    "    if len(clean_data) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate histogram and probabilities\n",
    "    hist, _ = np.histogram(clean_data, bins=bins)\n",
    "    if hist.sum() == 0:\n",
    "        return 0.0\n",
    "    probs = hist / hist.sum()\n",
    "    \n",
    "    # Calculate entropy, adding a small epsilon to avoid log(0)\n",
    "    return -np.sum(probs * np.log2(probs + 1e-10))\n",
    "\n",
    "def calculate_extreme_counts(data):\n",
    "    \"\"\"Calculate 3-sigma outlier counts.\"\"\"\n",
    "    if data.size < 2: return 0\n",
    "    mean, std = np.nanmean(data), np.nanstd(data)\n",
    "    if std == 0: return 0\n",
    "    return np.sum((data > (mean + 3*std)) | (data < (mean - 3*std)))\n",
    "\n",
    "def calculate_basic_stats(data, prefix):\n",
    "    \"\"\"Calculate basic statistical features from a 1D sensor array.\"\"\"\n",
    "    if data.size == 0: return {}\n",
    "    return {\n",
    "        f'(St)_{prefix}_mean': np.nanmean(data),\n",
    "        f'(St)_{prefix}_std': np.nanstd(data),\n",
    "        f'(St)_{prefix}_min': np.nanmin(data),\n",
    "        f'(St)_{prefix}_max': np.nanmax(data),\n",
    "        f'(St)_{prefix}_ptp': np.ptp(data),\n",
    "        f'(St)_{prefix}_skew': stats.skew(data, nan_policy='omit'),\n",
    "        f'(St)_{prefix}_kurtosis': stats.kurtosis(data, nan_policy='omit'),\n",
    "        f'(St)_{prefix}_mad': np.nanmedian(np.abs(data - np.nanmedian(data))),\n",
    "        f'(St)_{prefix}_extreme_count': calculate_extreme_counts(data),\n",
    "        f'(St)_{prefix}_entropy': calculate_entropy(data)\n",
    "    }\n",
    "\n",
    "def calculate_glcm_features(grid, prefix):\n",
    "    \"\"\"Calculate texture features using GLCM.\"\"\"\n",
    "    features = {}\n",
    "    props_to_calc = ['contrast', 'homogeneity', 'energy', 'correlation', 'entropy']\n",
    "    null_features = {f'(Sp)_{prefix}_glcm_{p}_mean': 0.0 for p in props_to_calc}\n",
    "    null_features.update({f'(Sp)_{prefix}_glcm_{p}_std': 0.0 for p in props_to_calc})\n",
    "    \n",
    "    valid_data = grid[grid != 0]\n",
    "    if valid_data.size < 2: return null_features\n",
    "\n",
    "    glcm_img = np.zeros_like(grid, dtype=np.uint8)\n",
    "    scaled = np.uint8((valid_data - valid_data.min()) / (valid_data.max() - valid_data.min() + 1e-8) * (GLCM_LEVELS - 1))\n",
    "    glcm_img[grid != 0] = scaled\n",
    "    \n",
    "    glcm = graycomatrix(glcm_img, GLCM_DISTANCES, GLCM_ANGLES, levels=GLCM_LEVELS, symmetric=True, normed=True)\n",
    "    \n",
    "    for prop in ['contrast', 'homogeneity', 'energy', 'correlation']:\n",
    "        values = graycoprops(glcm, prop)\n",
    "        features[f'(Sp)_{prefix}_glcm_{prop}_mean'] = values.mean()\n",
    "        features[f'(Sp)_{prefix}_glcm_{prop}_std'] = values.std()\n",
    "    \n",
    "    glcm_entropy = -np.sum(glcm * np.log2(glcm + 1e-10), axis=(0, 1))\n",
    "    features[f'(Sp)_{prefix}_glcm_entropy_mean'] = glcm_entropy.mean()\n",
    "    features[f'(Sp)_{prefix}_glcm_entropy_std'] = glcm_entropy.std()\n",
    "\n",
    "    return features\n",
    "\n",
    "def calculate_hotspot_features(grid, valid_data, prefix, mode, percentile):\n",
    "    \"\"\"Compute cluster-based features for hot or cold spots.\"\"\"\n",
    "    features = {\n",
    "        f'(Sp)_{prefix}_{mode}_clusters': 0, f'(Sp)_{prefix}_{mode}_abs_contrast': 0.0,\n",
    "        f'(Sp)_{prefix}_{mode}_rel_contrast': 0.0, f'(Sp)_{prefix}_{mode}_rel_size': 0.0,\n",
    "    }\n",
    "    if valid_data.size < DBSCAN_MIN_SAMPLES: return features\n",
    "\n",
    "    global_intensity = valid_data.mean()\n",
    "    threshold = np.percentile(valid_data, percentile)\n",
    "    mask = (grid > threshold) if mode == 'hot' else (grid < threshold) & (grid != 0)\n",
    "\n",
    "    coords = np.argwhere(mask)\n",
    "    if len(coords) < DBSCAN_MIN_SAMPLES: return features\n",
    "\n",
    "    labels = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit_predict(coords)\n",
    "    unique_labels = set(labels) - {-1}\n",
    "    if not unique_labels: return features\n",
    "\n",
    "    cluster_sizes = [np.sum(labels == lbl) for lbl in unique_labels]\n",
    "    cluster_intensities = [grid[coords[labels == lbl, 0], coords[labels == lbl, 1]].mean() for lbl in unique_labels]\n",
    "    \n",
    "    abs_contrast = (np.median(cluster_intensities) - global_intensity) if mode == 'hot' else (global_intensity - np.median(cluster_intensities))\n",
    "\n",
    "    features[f'(Sp)_{prefix}_{mode}_clusters'] = len(unique_labels)\n",
    "    features[f'(Sp)_{prefix}_{mode}_abs_contrast'] = abs_contrast\n",
    "    features[f'(Sp)_{prefix}_{mode}_rel_contrast'] = safe_divide(abs_contrast, global_intensity)\n",
    "    features[f'(Sp)_{prefix}_{mode}_rel_size'] = safe_divide(max(cluster_sizes), mask.sum())\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_derived_features(features):\n",
    "    \"\"\"Calculate cross-channel derived features.\"\"\"\n",
    "    total_sensor_energy = features.get('(St)_MeltVIEW plasma_mean', 0) + features.get('(St)_MeltVIEW melt pool_mean', 0)\n",
    "    \n",
    "    return {\n",
    "        '(PD)_Total_Energy': total_sensor_energy,\n",
    "        '(PD)_MVP/TE_ratio': safe_divide(features.get('(St)_MeltVIEW plasma_mean', 0), total_sensor_energy),\n",
    "        '(PD)_MVP/MVMP_ratio': safe_divide(features.get('(St)_MeltVIEW plasma_mean', 0), features.get('(St)_MeltVIEW melt pool_mean', 0)),\n",
    "        '(PD)_MVMP/LV': safe_divide(features.get('(St)_MeltVIEW melt pool_mean', 0), features.get('(St)_LaserVIEW_mean', 0)),\n",
    "        '(PD)_MVP/LV': safe_divide(features.get('(St)_MeltVIEW plasma_mean', 0), features.get('(St)_LaserVIEW_mean', 0)),\n",
    "        '(PD)_LV_STD/LV': safe_divide(features.get('(St)_LaserVIEW_std', 0), features.get('(St)_LaserVIEW_mean', 0)),\n",
    "        '(PDPB)_MVMP/VED': safe_divide(features.get('(St)_MeltVIEW melt pool_mean', 0), features.get('(PB)_VED', 0)),\n",
    "        '(PDPB)_MVP/VED': safe_divide(features.get('(St)_MeltVIEW plasma_mean', 0), features.get('(PB)_VED', 0)),\n",
    "        '(PDPB)_TotalEnergy/VED': safe_divide(total_sensor_energy, features.get('(PB)_VED', 0)),\n",
    "        '(PDPB)_LV/LP': safe_divide(features.get('(St)_LaserVIEW_mean', 0), features.get('(PB)_laser_power', 0))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391363ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cell 4: Main Processing Function '''\n",
    "# =============================================================================\n",
    "def process_layer(layer_path, meta_row):\n",
    "    \"\"\"\n",
    "    Orchestrates the feature calculation for a single layer CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(layer_path)\n",
    "    coupon_id = meta_row['Coupon']\n",
    "    layer_num = int(re.search(r'layer_(\\d+)', layer_path).group(1))\n",
    "\n",
    "    # Initialize features with metadata\n",
    "    features = {\n",
    "        'coupon_id': coupon_id,\n",
    "        'layer_index': layer_num,\n",
    "        'Label': meta_row['Label'],\n",
    "        '(MT)_identifier': meta_row['Identifier'],\n",
    "        '(MT)_center_x': meta_row['Center_X'],\n",
    "        '(MT)_center_y': meta_row['Center_Y'],\n",
    "        '(PB)_laser_power': meta_row['Laser Power'],\n",
    "        '(PB)_scan_speed': meta_row['Scan Speed'],\n",
    "        '(PB)_hatch_distance': meta_row['Hatch Distance'],\n",
    "        '(PB)_powder_thickness': meta_row['Powder Thickness'],\n",
    "        '(PB)_VED': meta_row['VED']\n",
    "    }\n",
    "    \n",
    "    # Create interpolated grid\n",
    "    x_coords = np.linspace(meta_row['Center_X'] - 7.5, meta_row['Center_X'] + 7.5, GRID_SIZE)\n",
    "    y_coords = np.linspace(meta_row['Center_Y'] + 7.5, meta_row['Center_Y'] - 7.5, GRID_SIZE)\n",
    "    grid = np.zeros((GRID_SIZE, GRID_SIZE, len(SENSOR_CHANNELS)), dtype=np.float32)\n",
    "\n",
    "    for i, channel in enumerate(SENSOR_CHANNELS):\n",
    "        grid[:, :, i] = griddata(\n",
    "            (df[\"Demand X\"], df[\"Demand Y\"]), df[channel].values,\n",
    "            (x_coords[None, :], y_coords[:, None]), method='linear', fill_value=0\n",
    "        )\n",
    "    \n",
    "    for i, channel in enumerate(SENSOR_CHANNELS):\n",
    "        data = df[channel].values\n",
    "        features.update(calculate_basic_stats(data, channel))\n",
    "        \n",
    "        grid_channel = grid[:, :, i]\n",
    "        valid_grid_data = grid_channel[grid_channel != 0]\n",
    "\n",
    "        if valid_grid_data.size > 1 and channel in ['MeltVIEW plasma', 'MeltVIEW melt pool']:\n",
    "            features.update(calculate_glcm_features(grid_channel, channel))\n",
    "            features.update(calculate_hotspot_features(grid_channel, valid_grid_data, channel, 'hot', DBSCAN_HOTSPOT_PERCENTILE))\n",
    "            features.update(calculate_hotspot_features(grid_channel, valid_grid_data, channel, 'cold', DBSCAN_COLDSPOT_PERCENTILE))\n",
    "\n",
    "    features.update(calculate_derived_features(features))\n",
    "    \n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cell 5: Main Script Execution'''\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    try:\n",
    "        metadata_df = pd.read_csv(METADATA_PATH)\n",
    "        metadata_df.columns = metadata_df.columns.str.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not load metadata file. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    all_features_list = []\n",
    "    \n",
    "    layer_file_paths = []\n",
    "    for dirpath, _, filenames in os.walk(INTERIM_ROOT_DIR):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.csv'):\n",
    "                layer_file_paths.append(os.path.join(dirpath, f))\n",
    "\n",
    "    if not layer_file_paths:\n",
    "        print(\"No segmented layer files found in 'data/interim/'. Please run the segmentation script first.\")\n",
    "        return\n",
    "\n",
    "    for layer_path in tqdm(layer_file_paths, desc=\"Generating Features\"):\n",
    "        try:\n",
    "            coupon_id = os.path.basename(os.path.dirname(layer_path))\n",
    "            meta_row = metadata_df[metadata_df['Coupon'] == coupon_id].iloc[0]\n",
    "            features = process_layer(layer_path, meta_row)\n",
    "            all_features_list.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not process {layer_path}. Error: {e}\")\n",
    "\n",
    "    if not all_features_list:\n",
    "        print(\"No features were generated.\")\n",
    "        return\n",
    "        \n",
    "    final_df = pd.DataFrame(all_features_list)\n",
    "    \n",
    "    final_model_columns = [\n",
    "        'coupon_id', 'layer_index', 'Label',\n",
    "        '(St)_MeltVIEW plasma_mean', '(St)_MeltVIEW plasma_std', '(St)_MeltVIEW plasma_min', '(St)_MeltVIEW plasma_max', '(St)_MeltVIEW plasma_ptp', '(St)_MeltVIEW plasma_skew', '(St)_MeltVIEW plasma_kurtosis', '(St)_MeltVIEW plasma_mad', '(St)_MeltVIEW plasma_extreme_count', '(St)_MeltVIEW plasma_entropy', '(Sp)_MeltVIEW plasma_glcm_contrast_mean', '(Sp)_MeltVIEW plasma_glcm_contrast_std', '(Sp)_MeltVIEW plasma_glcm_homogeneity_mean', '(Sp)_MeltVIEW plasma_glcm_homogeneity_std', '(Sp)_MeltVIEW plasma_glcm_energy_mean', '(Sp)_MeltVIEW plasma_glcm_energy_std', '(Sp)_MeltVIEW plasma_glcm_correlation_mean', '(Sp)_MeltVIEW plasma_glcm_correlation_std', '(Sp)_MeltVIEW plasma_glcm_entropy_mean', '(Sp)_MeltVIEW plasma_glcm_entropy_std', '(Sp)_MeltVIEW plasma_hot_clusters', '(Sp)_MeltVIEW plasma_hot_abs_contrast', '(Sp)_MeltVIEW plasma_hot_rel_contrast', '(Sp)_MeltVIEW plasma_hot_rel_size', '(Sp)_MeltVIEW plasma_cold_clusters', '(Sp)_MeltVIEW plasma_cold_abs_contrast', '(Sp)_MeltVIEW plasma_cold_rel_contrast', '(Sp)_MeltVIEW plasma_cold_rel_size',\n",
    "        '(St)_MeltVIEW melt pool_mean', '(St)_MeltVIEW melt pool_std', '(St)_MeltVIEW melt pool_min', '(St)_MeltVIEW melt pool_max', '(St)_MeltVIEW melt pool_ptp', '(St)_MeltVIEW melt pool_skew', '(St)_MeltVIEW melt pool_kurtosis', '(St)_MeltVIEW melt pool_mad', '(St)_MeltVIEW melt pool_extreme_count', '(St)_MeltVIEW melt pool_entropy', '(Sp)_MeltVIEW melt pool_glcm_contrast_mean', '(Sp)_MeltVIEW melt pool_glcm_contrast_std', '(Sp)_MeltVIEW melt pool_glcm_homogeneity_mean', '(Sp)_MeltVIEW melt pool_glcm_homogeneity_std', '(Sp)_MeltVIEW melt pool_glcm_energy_mean', '(Sp)_MeltVIEW melt pool_glcm_energy_std', '(Sp)_MeltVIEW melt pool_glcm_correlation_mean', '(Sp)_MeltVIEW melt pool_glcm_correlation_std', '(Sp)_MeltVIEW melt pool_glcm_entropy_mean', '(Sp)_MeltVIEW melt pool_glcm_entropy_std', '(Sp)_MeltVIEW melt pool_hot_clusters', '(Sp)_MeltVIEW melt pool_hot_abs_contrast', '(Sp)_MeltVIEW melt pool_hot_rel_contrast', '(Sp)_MeltVIEW melt pool_hot_rel_size', '(Sp)_MeltVIEW melt pool_cold_clusters', '(Sp)_MeltVIEW melt pool_cold_abs_contrast', '(Sp)_MeltVIEW melt pool_cold_rel_contrast', '(Sp)_MeltVIEW melt pool_cold_rel_size',\n",
    "        '(St)_LaserVIEW_mean', '(St)_LaserVIEW_std', '(St)_LaserVIEW_min', '(St)_LaserVIEW_max', '(St)_LaserVIEW_ptp', '(St)_LaserVIEW_skew', '(St)_LaserVIEW_kurtosis', '(St)_LaserVIEW_mad', '(St)_LaserVIEW_extreme_count', '(St)_LaserVIEW_entropy',\n",
    "        '(PD)_Total_Energy', '(PD)_MVP/TE_ratio', '(PD)_MVP/MVMP_ratio', '(PD)_MVMP/LV', '(PD)_MVP/LV', '(PD)_LV_STD/LV',\n",
    "        '(PDPB)_MVMP/VED', '(PDPB)_MVP/VED', '(PDPB)_TotalEnergy/VED', '(PDPB)_LV/LP'\n",
    "    ]\n",
    "    \n",
    "    final_df = final_df[final_model_columns]\n",
    "    \n",
    "    final_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"\\n✅ Processing complete. Saved {len(final_df)} rows and {len(final_df.columns)} columns to '{OUTPUT_CSV_PATH}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6912dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cell 6: Run Main Script'''\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
